% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CISC 7021 Applied Natural Language Processing - Review Report}



\author{HOI HOU HONG \\
  MC45134 \\
  \texttt{mc45134@um.edu.mo} \\\And
  CHEN SIO IN \\
  MC14988\\
  \texttt{mc14988@um.edu.mo} \\\And
  LAO KA SENG \\
  MC45057 \\
  \texttt{mc45057@um.edu.mo} \\}




\begin{document}
\maketitle
\begin{abstract}
This coursework aims to recreate several classic programming projects by leveraging the capabilities of Large Language Models (LLMs). To achieve this objective, we have undertaken a literature review to identify optimal approaches and methodologies for implementing such tasks through querying LLMs. Our goal is to understand the effectiveness of LLMs in generating functional code and to establish best practices for utilizing these models in educational settings.
\end{abstract}

\section{Introduction}

The objective of this coursework is to recreate a selection of classic programming projects utilizing Large Language Models (LLMs) through query-based interactions. To gain insight into effective methodologies and practices, we will conduct a thorough review of relevant literature, including seminal works such as 'A Survey on Evaluating Large Language Models in Code Generation Tasks.'\cite{chen2024survey} This work elucidates the current state of evaluating LLMs in the realm of code generation, providing us with foundational knowledge on assessing the performance and applicability of these models. 


Additionally, we will examine 'XXXXXXX' [Note: Insert the actual title of the second reference here], which further contributes to our understanding by highlighting [specific contribution or method discussed in the second reference]. Through these efforts, we aim to establish a robust framework for leveraging LLMs in recreating classic programming projects, thereby exploring the potential of advanced language models in educational and practical coding contexts.

\section{Tentative Code project}
In this coursework, we propose to undertake a series of tentative code projects aimed at evaluating the capabilities of Large Language Models (LLMs) in generating functional code. Specifically, we intend to explore the performance of LLMs across three domains: 

1.developing a simple game, 

2.constructing a basic database system, and 

3.creating a rudimentary recommendation system. 

By engaging LLMs in these tasks, we seek to assess their proficiency in generating syntactically correct, semantically meaningful, and functionally effective code. The evaluation will encompass metrics such as code correctness, efficiency, and readability, providing a comprehensive overview of the models' performance in practical software development scenarios.

\section{Experiments}

A List of Papers, Including References
To gain a fundamental understanding of Large Language Models (LLMs) and their application in code generation tasks, I have read the following papers:


\begin{verbatim}

Chen, L., Guo, Q., Jia, H., Zeng, Z., Wang, X., Xu, Y., ... & Zhang, S. (2024). A Survey on Evaluating Large Language Models in Code Generation Tasks. Journal of Computer Science and Technology.
Ni, A., Yin, P., Zhao, Y., Riddell, M., Feng, T., Shen, R., ... & Xiong, C. (2023). L2ceval: Evaluating language-to-code generation capabilities of large language models. arXiv e-prints, pages arXiv–2309.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.
Puri, R., Kung, D.S., Janssen, G., Zhang, W., Domeniconi, G., Zolotov, V., Dolby, J., Chen, J., Choudhury, M., Decker, L., Thost, V., Buratti, L., Pujar, S., Ramji, S., Finkler, U., Malaika, S., & Reiss, F. (2021). Codenet: A large-scale AI for code dataset for learning a diversity of coding tasks.
Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., & Ma, S. (2020). CodeBLEU: A method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297.
Wang, J., Cao, L., Luo, X., Zhou, Z., Xie, J., Jatowt, A., & Cai, Y. (2023). Enhancing large language models for secure code generation: A dataset-driven study on vulnerability mitigation. arXiv e-prints, pages arXiv–2310.
Wang, X., Wang, Y., Wan, Y., Mi, F., Li, Y., Zhou, P., Liu, J., Wu, H., Jiang, X., & Liu, Q. (2022). Compilable neural code generation with compiler feedback. arXiv preprint arXiv:2203.05132.

\end{verbatim}

\section{Main Methodologies Previously Used
The main methodologies utilized in previous studies include:}


Compilation Success Rate: Measuring the ability of the generated code to compile without errors.
Unit Test Pass Rate: Determining the percentage of generated code that passes predefined unit tests.
Performance and Efficiency Metrics: Evaluating the computational efficiency and resource utilization of the generated code.
Expert Review: Soliciting feedback from domain experts regarding the quality and usability of the generated code.
User Experience: Collecting user feedback on the generated code's readability and maintainability.
Experimental Results Obtained
From the aforementioned works, the experimental results indicate that while LLMs demonstrate strong performance in generating functional code, they face challenges in non-functional aspects such as generating high-quality test cases and detecting defects. Continuous pre-training and instruction fine-tuning have shown mixed effects, suggesting the necessity for strategic selection of training methods. Comprehensive assessments reveal varying performance across different tasks, highlighting the need for targeted strategies to enhance LLM capabilities in software engineering contexts.

\section{Tentative Schedule}

Here is a proposed schedule for completing the tasks within one month:


Week 1: Literature review and setup of the testing environment, including selection of appropriate LLMs.

Week 2: Code generation for simple games using selected LLMs and initial testing.

Week 3: Code generation for database systems and testing.

Week 4: Code generation for recommendation systems followed by final testing and compilation of results.

Week 5: Data analysis, report writing, and preparation of the final document.

By adhering to this timeline, it is feasible to conduct thorough evaluations of LLMs across three distinct areas—simple games, database systems, and recommendation systems—within the given timeframe.



 
    





\pagebreak






\section*{Use of Generative AI}
I acknowledge the use of [ Qwen2.5,GPT4o and https://chatgpt.com] to help me finish coding and specific term analysis.\cite{touvron2023llama}

I declare that the graphs, result of zero-task and fine-tuned, F1 score,  generated by tiny Llama 2 has been presented and submitted as my own work.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix





\end{document}
